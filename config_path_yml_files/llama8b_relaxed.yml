schedulers:
  # - name: vllm
  #   scheduler: vllm
  #   batch_size: 128
  # - name: orca
  #   scheduler: orca
  #   batch_size: 8 # with higher batch size activation and kv cache memory blows up
  # - name: sarathi_spf_512
  #   scheduler: sarathi
  #   chunk_size: 512
  #   batch_size: 128
  # - name: slai_scheduler_fcfs_fixed_offset
  #   scheduler: slai_scheduler
  #   token_budget: 512
  #   batch_size: 128
  #   limit_total_decodes: 128
  #   time_between_tokens: 0.2
  #   fcfs : true
  #   fixed_offset: true
  #   below_memory_limit_offset : 10 
  #   above_memory_limit_offset : 10
  #   memory_limit : 1 
  #   user_priority : false
  # - name: slai_scheduler_spf_fixed_offset
  #   scheduler: slai_scheduler
  #   token_budget: 512
  #   batch_size: 128
  #   limit_total_decodes: 128
  #   time_between_tokens: 0.2
  #   fcfs : false
  #   fixed_offset: true
  #   below_memory_limit_offset : 10 
  #   above_memory_limit_offset : 10
  #   memory_limit : 1
  #   user_priority : false
  # - name: slai_scheduler_spf_dynamic_offset
  #   scheduler: slai_scheduler
  #   token_budget: 512
  #   batch_size: 128
  #   limit_total_decodes: 128
  #   time_between_tokens: 0.2
  #   fcfs : false
  #   fixed_offset: false
  #   below_memory_limit_offset : 5
  #   above_memory_limit_offset : 10
  #   memory_limit : 0.96
  #   user_priority : false
  - name: slai_scheduler_spf_dynamic_offset_user_priority
    scheduler: slai_scheduler
    token_budget: 512
    batch_size: 128
    limit_total_decodes: 128
    time_between_tokens: 0.2
    fcfs : false
    fixed_offset: false
    below_memory_limit_offset : 5
    above_memory_limit_offset : 10
    memory_limit : 0.96
    user_priority : true


traces:
  - name: chat
    trace_file: "./data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv"
    max_seq_len: 8192
    num_requests: 16384
    start_qps: 2

parallel_spec:
  - name: tp_1        
    tp_dimension: 1
    pp_dimension: 1

models:
  - name: llama-3.1-8b
    identifier: meta-llama/Llama-3.1-8B-Instruct
    parallel_specs: ["tp_1"]
    scheduler_specs: null # usee all
    traces: null # use all
