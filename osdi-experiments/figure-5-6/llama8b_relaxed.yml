schedulers:
  # - name: vllm
  #   scheduler: vllm
  #   batch_size: 128
  # - name: orca
  #   scheduler: orca
  #   batch_size: 8 # with higher batch size activation and kv cache memory blows up
  # - name: sarathi_spf_512
  #   scheduler: sarathi
  #   chunk_size: 512
  #   batch_size: 128
  # - name: last_minute_cs512_bs128_offset_10_limittotaldecodes_128
  #   scheduler: last_minute
  #   token_budget: 512
  #   batch_size: 128
  #   offset: 10
  #   limit_total_decodes: 128
  #   time_between_tokens: 0.2 
  #   process_smallest_prefill: false
  #   capacity: 2 
  - name: experimental_scheduler_shortest_job_first_dynamic_offset
    scheduler: experimental_scheduler
    token_budget: 512
    batch_size: 128
    offset: 5
    limit_total_decodes: 128
    time_between_tokens: 0.1
    capacity: 1

traces:
  - name: chat
    trace_file: "./data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv"
    max_seq_len: 8192
    num_requests: 16384
    start_qps: 2
  # - name: arxiv
  #   trace_file: "./data/processed_traces/arxiv_summarization_filtered_stats_llama2_tokenizer.csv"
  #   max_seq_len: 16384
  #   num_requests: 2048
  #   start_qps: 0.5

parallel_spec:
  - name: tp_1        
    tp_dimension: 1
    pp_dimension: 1

models:
  - name: llama-3.1-8b
    identifier: meta-llama/Llama-3.1-8B-Instruct
    parallel_specs: ["tp_1"]
    scheduler_specs: null # usee all
    traces: null # use all
