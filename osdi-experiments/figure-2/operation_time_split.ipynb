{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import uuid\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_constructor(loader, node):\n",
    "    return tuple(loader.construct_sequence(node))\n",
    "\n",
    "\n",
    "yaml.add_constructor(\"tag:yaml.org,2002:python/tuple\", tuple_constructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f\"{Path.cwd()}/benchmark_output\"\n",
    "PREFILL_TRACE_FILE = f\"{Path.cwd()}/prefill_operation_time_split_experiment_trace.csv\"\n",
    "MODEL_NAME = \"01-ai/Yi-34B-200K\"\n",
    "MODEL_TP_DIMENSION = 2\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-70b-hf\"\n",
    "# MODEL_TP_DIMENSION = 4\n",
    "\n",
    "SEQUENCE_LENGTHS = [128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "PREFILL_EXPERIMENT_BATCH_SIZE = 1\n",
    "NUM_REQUESTS_PER_SEQUENCE_LENGTH = 1\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1024\n",
    "NUM_DECODE_TOKENS = 4\n",
    "DECODE_BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "WANDB_PROJECT = \"llm-simulator\"\n",
    "WANDB_GROUP = \"vllm_operation_time_split_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trace_file_for_prefill_experiment():\n",
    "    with open(PREFILL_TRACE_FILE, \"w\") as prefill_experiment_trace_file:\n",
    "        prefill_experiment_trace_file.write(\n",
    "            \"num_prefill_tokens,num_decode_tokens,num_total_tokens,pd_ratio\\n\"\n",
    "        )\n",
    "        for sequence_length in SEQUENCE_LENGTHS:\n",
    "            for _ in range(NUM_REQUESTS_PER_SEQUENCE_LENGTH):\n",
    "                prefill_experiment_trace_file.write(\n",
    "                    f\"{sequence_length},1,{sequence_length},{sequence_length}\\n\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_random_suffix():\n",
    "    return str(uuid.uuid4()).split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_runs():\n",
    "    devices = \",\".join([str(x) for x in range(MODEL_TP_DIMENSION)])\n",
    "    generate_trace_file_for_prefill_experiment()\n",
    "    prefill_experiment_run_name = f\"{MODEL_NAME.split('/')[-1]}_tp_{MODEL_TP_DIMENSION}_prefill_split_{_get_random_suffix()}\"\n",
    "    command = (\n",
    "        f\"CUDA_VISIBLE_DEVICES={devices} python sarathi/benchmark/main.py \\\\\\n\"\n",
    "        f\"--output_dir {OUTPUT_DIR} \\\\\\n\"\n",
    "        f\"--model_name {MODEL_NAME} \\\\\\n\"\n",
    "        f\"--model_max_model_len {max(SEQUENCE_LENGTHS) + 1} \\\\\\n\"\n",
    "        f\"--cluster_num_replicas 1 \\\\\\n\"\n",
    "        f\"--model_tensor_parallel_degree {MODEL_TP_DIMENSION} \\\\\\n\"\n",
    "        f\"--model_pipeline_parallel_degree 1 \\\\\\n\"\n",
    "        f\"--request_generator_provider synthetic \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_length_provider synthetic \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_length_provider trace \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_interval_provider static \\\\\\n\"\n",
    "        f\"--trace_request_length_generator_trace_file {PREFILL_TRACE_FILE} \\\\\\n\"\n",
    "        f\"--trace_request_length_generator_max_tokens {max(SEQUENCE_LENGTHS) + 1} \\\\\\n\"\n",
    "        f\"--trace_request_length_generator_prefill_scale_factor 1 \\\\\\n\"\n",
    "        f\"--trace_request_length_generator_decode_scale_factor 1 \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_num_requests {NUM_REQUESTS_PER_SEQUENCE_LENGTH * len(SEQUENCE_LENGTHS)} \\\\\\n\"\n",
    "        f\"--metrics_store_keep_individual_batch_metrics true \\\\\\n\"\n",
    "        f\"--replica_scheduler_provider vllm \\\\\\n\"\n",
    "        f\"--replica_scheduler_max_batch_size 1 \\\\\\n\"\n",
    "        f\"--vllm_scheduler_max_tokens_in_batch {max(SEQUENCE_LENGTHS) + 1} \\\\\\n\"\n",
    "        f\"--metrics_store_wandb_project {WANDB_PROJECT} \\\\\\n\"\n",
    "        f\"--metrics_store_wandb_group {WANDB_GROUP} \\\\\\n\"\n",
    "        f\"--metrics_store_wandb_run_name {prefill_experiment_run_name} \\\\\\n\"\n",
    "    )\n",
    "    commands = [\n",
    "        command + f\"--metrics_store_enable_op_level_metrics false \\n\",\n",
    "        command + f\"--metrics_store_enable_op_level_metrics true \\n\",\n",
    "    ]\n",
    "    for batch_size in DECODE_BATCH_SIZES:\n",
    "        decode_run_name = f\"{MODEL_NAME.split('/')[-1]}_tp_{MODEL_TP_DIMENSION}_seq_len_{MAX_SEQUENCE_LENGTH}_decode_batch_size_{batch_size}_decode_split_{_get_random_suffix()}\"\n",
    "\n",
    "        command = (\n",
    "            f\"CUDA_VISIBLE_DEVICES={devices} python sarathi/benchmark/main.py \\\\\\n\"\n",
    "            f\"--output_dir {OUTPUT_DIR} \\\\\\n\"\n",
    "            f\"--model_name {MODEL_NAME} \\\\\\n\"\n",
    "            f\"--model_max_model_len {MAX_SEQUENCE_LENGTH} \\\\\\n\"\n",
    "            f\"--cluster_num_replicas 1 \\\\\\n\"\n",
    "            f\"--model_tensor_parallel_degree {MODEL_TP_DIMENSION} \\\\\\n\"\n",
    "            f\"--model_pipeline_parallel_degree 1 \\\\\\n\"\n",
    "            f\"--request_generator_provider synthetic \\\\\\n\"\n",
    "            f\"--synthetic_request_generator_length_provider synthetic \\\\\\n\"\n",
    "            f\"--synthetic_request_generator_length_provider uniform \\\\\\n\"\n",
    "            f\"--synthetic_request_generator_interval_provider static \\\\\\n\"\n",
    "            f\"--uniform_request_length_generator_prefill_to_decode_ratio {MAX_SEQUENCE_LENGTH / NUM_DECODE_TOKENS - 1} \\\\\\n\"\n",
    "            f\"--uniform_request_length_generator_min_tokens {MAX_SEQUENCE_LENGTH} \\\\\\n\"\n",
    "            f\"--uniform_request_length_generator_max_tokens {MAX_SEQUENCE_LENGTH} \\\\\\n\"\n",
    "            f\"--synthetic_request_generator_num_requests {2 * batch_size} \\\\\\n\"\n",
    "            f\"--metrics_store_keep_individual_batch_metrics true \\\\\\n\"\n",
    "            f\"--replica_scheduler_provider vllm \\\\\\n\"\n",
    "            f\"--replica_scheduler_max_batch_size {batch_size} \\\\\\n\"\n",
    "            f\"--vllm_scheduler_max_tokens_in_batch {MAX_SEQUENCE_LENGTH} \\\\\\n\"\n",
    "            f\"--metrics_store_wandb_project {WANDB_PROJECT} \\\\\\n\"\n",
    "            f\"--metrics_store_wandb_group {WANDB_GROUP} \\\\\\n\"\n",
    "            f\"--metrics_store_wandb_run_name {decode_run_name} \\\\\\n\"\n",
    "        )\n",
    "        commands.append(command + f\"--metrics_store_enable_op_level_metrics false \\n\")\n",
    "        commands.append(command + f\"--metrics_store_enable_op_level_metrics true \\n\")\n",
    "\n",
    "    with open(f\"operation_time_split_runs.sh\", \"w\") as f:\n",
    "        f.write(\"#!/bin/bash\\n\")\n",
    "        f.write(\"set -x\\n\")\n",
    "        for command in commands:\n",
    "            f.write(command + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_run_directories():\n",
    "    rootdir = Path(OUTPUT_DIR)\n",
    "    subdirectory_list = [\n",
    "        directory for directory in rootdir.iterdir() if directory.is_dir()\n",
    "    ]\n",
    "    return sorted(subdirectory_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_df_for_single_run(\n",
    "    config: Dict[str, object],\n",
    "    benchmark_config: Dict[str, object],\n",
    "    batch_metrics: pd.DataFrame,\n",
    "    operation_metrics: pd.DataFrame,\n",
    "    batch_num_prefill_tokens: int,\n",
    "    batch_num_decode_tokens: int,\n",
    "    batch_num_tokens: int,\n",
    "):\n",
    "    df = batch_metrics.copy()\n",
    "    # df = pd.merge(batch_metrics, operation_metrics, on=[\"Batch Id\"], how=\"inner\")\n",
    "\n",
    "    # print(len(df))\n",
    "    df = df[\n",
    "        (df[\"batch_num_tokens\"] == batch_num_tokens)\n",
    "        & (df[\"batch_num_prefill_tokens\"] == batch_num_prefill_tokens)\n",
    "        & (df[\"batch_num_decode_tokens\"] == batch_num_decode_tokens)\n",
    "    ]\n",
    "\n",
    "    df[\"operation\"] = \"batch_execution_time\"\n",
    "    df[\"operation_time_per_token\"] = (df[\"batch_execution_time\"] * 1000) / df[\n",
    "        \"batch_num_tokens\"\n",
    "    ]\n",
    "    # print(df)\n",
    "    return df\n",
    "\n",
    "    # datapoints = df.to_dict('records')\n",
    "    # final_datapoints = []\n",
    "    # for datapoint in datapoints:\n",
    "    #     point = datapoint.copy()\n",
    "    #     linear_time = point[\"mlp_up_proj\"] + point[\"mlp_down_proj\"] + point[\"attn_pre_proj\"] + point[\"attn_post_proj\"]\n",
    "    #     attention_time = point[\"attn_prefill\"] + point[\"attn_decode\"]\n",
    "    #     communication_time = point[\"mlp_up_proj_all_gather\"] + point[\"mlp_down_proj_all_reduce\"] + point[\"attn_pre_proj_all_gather\"] + point[\"attn_post_proj_all_reduce\"] + point[\"embed_all_reduce\"]\n",
    "    #     others_time = point[\"model_compute_time\"] - linear_time - attention_time - communication_time\n",
    "    #     final_datapoints.extend([\n",
    "    #         { **point, \"operation\": \"linear\", \"operation_time\": linear_time },\n",
    "    #         { **point, \"operation\": \"communication\", \"operation_time\": communication_time },\n",
    "    #         { **point, \"operation\": \"attention\", \"operation_time\": attention_time },\n",
    "    #         { **point, \"operation\": \"others\", \"operation_time\": others_time },\n",
    "    #     ])\n",
    "    # return pd.DataFrame(final_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_df():\n",
    "    run_directories = _get_run_directories()\n",
    "    run_directories = run_directories[-1:]\n",
    "\n",
    "\n",
    "    # num_runs = 2 * (len(DECODE_BATCH_SIZES) + 1)\n",
    "    # run_directories = run_directories[-num_runs:]\n",
    "\n",
    "    prefill_dfs = []\n",
    "    decode_dfs = []\n",
    "    yaml.SafeLoader.add_constructor(\"tag:yaml.org,2002:python/tuple\", tuple_constructor)\n",
    "\n",
    "    for run_dir in run_directories:\n",
    "        with open(f\"{run_dir}/replica_0/config.yml\", \"r\") as config_file, open(\n",
    "            f\"{run_dir}/benchmark_config.yml\", \"r\"\n",
    "        ) as benchmark_config_file, open(\n",
    "            f\"{run_dir}/replica_0/batch_metrics.csv\", \"r\"\n",
    "        ) as batch_metrics_file:\n",
    "            config = yaml.safe_load(config_file)\n",
    "            benchmark_config = yaml.safe_load(benchmark_config_file)\n",
    "            batch_metrics = pd.read_csv(batch_metrics_file)\n",
    "            # operation_metrics = pd.read_csv(operation_metrics_file)\n",
    "\n",
    "            if benchmark_config[\"metrics_store_enable_op_level_metrics\"] == True:\n",
    "                continue\n",
    "\n",
    "            if (\n",
    "                benchmark_config[\"trace_request_length_generator_trace_file\"]\n",
    "                == PREFILL_TRACE_FILE\n",
    "            ):\n",
    "                for seq_len in SEQUENCE_LENGTHS:\n",
    "                    prefill_dfs.append(\n",
    "                        _get_df_for_single_run(\n",
    "                            config,\n",
    "                            benchmark_config,\n",
    "                            batch_metrics,\n",
    "                            None,\n",
    "                            seq_len,\n",
    "                            0,\n",
    "                            seq_len,\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                batch_size = benchmark_config[\"replica_scheduler_max_batch_size\"]\n",
    "                decode_dfs.append(\n",
    "                    _get_df_for_single_run(\n",
    "                        config,\n",
    "                        benchmark_config,\n",
    "                        batch_metrics,\n",
    "                        None,\n",
    "                        0,\n",
    "                        batch_size,\n",
    "                        batch_size,\n",
    "                    )\n",
    "                )\n",
    "    prefill_df = pd.concat(prefill_dfs)\n",
    "    prefill_df = (\n",
    "        prefill_df.groupby(\n",
    "            [\n",
    "                \"batch_num_tokens\",\n",
    "                \"batch_num_decode_tokens\",\n",
    "                \"batch_num_prefill_tokens\",\n",
    "                \"operation\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    prefill_df.to_csv(\"prefill.csv\")\n",
    "    decode_df = pd.concat(decode_dfs)\n",
    "    decode_df = (\n",
    "        decode_df.groupby(\n",
    "            [\n",
    "                \"batch_num_tokens\",\n",
    "                \"batch_num_decode_tokens\",\n",
    "                \"batch_num_prefill_tokens\",\n",
    "                \"operation\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    decode_df.to_csv(\"decode.csv\")\n",
    "    return prefill_df, decode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    p_df, d_df = _get_df()\n",
    "    ops = [\"batch_execution_time\"]\n",
    "    # ops = ['attention', 'linear', 'allreduce', 'others']\n",
    "    prefill_record = dict()\n",
    "    decode_record = dict()\n",
    "\n",
    "    def load_prefill_data():\n",
    "        for iter, row in p_df.iterrows():\n",
    "            bs = row[\"batch_num_prefill_tokens\"]\n",
    "            op = row[\"operation\"]\n",
    "            time = row[\"operation_time_per_token\"]\n",
    "            if bs not in prefill_record:\n",
    "                prefill_record[bs] = dict()\n",
    "            if op not in prefill_record[bs]:\n",
    "                prefill_record[bs][op] = time\n",
    "            prefill_record[bs][op] = time\n",
    "\n",
    "    def load_decode_data():\n",
    "        for iter, row in d_df.iterrows():\n",
    "            bs = row[\"batch_num_decode_tokens\"]\n",
    "            op = row[\"operation\"]\n",
    "            time = row[\"operation_time_per_token\"]\n",
    "            if bs not in decode_record:\n",
    "                decode_record[bs] = dict()\n",
    "            if op not in decode_record[bs]:\n",
    "                decode_record[bs][op] = time\n",
    "            decode_record[bs][op] = time\n",
    "\n",
    "    load_prefill_data()\n",
    "    load_decode_data()\n",
    "\n",
    "    patterns = [\"//\", \"-\", \"\\\\\\\\\", \"\", \"x\", \"\"]\n",
    "    colors = sns.hls_palette(l=0.7)\n",
    "    opacity = 0.9\n",
    "    prefill_batch_sizes = [128, 256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "    decode_batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "    def get_bs_label(bs):\n",
    "        if bs == 1024:\n",
    "            return \"1K\"\n",
    "        elif bs == 2048:\n",
    "            return \"2K\"\n",
    "        elif bs == 4096:\n",
    "            return \"4K\"\n",
    "        elif bs == 8192:\n",
    "            return \"8K\"\n",
    "        elif bs == 16384:\n",
    "            return \"16K\"\n",
    "        return str(bs)\n",
    "\n",
    "    prefill_x_labels, decode_x_labels = [], []\n",
    "    for bs in prefill_batch_sizes:\n",
    "        p_time = 0\n",
    "        for op in ops:\n",
    "            p_time += prefill_record[bs][op] * bs\n",
    "        p_time = int(p_time)\n",
    "        prefill_x_labels.append(get_bs_label(bs) + f\" ({p_time}ms)\")\n",
    "        d_time = 0\n",
    "\n",
    "    for bs in decode_batch_sizes:\n",
    "        d_time = 0\n",
    "        for op in ops:\n",
    "            d_time += decode_record[bs][op] * bs\n",
    "        d_time = int(d_time)\n",
    "        decode_x_labels.append(str(bs) + f\" ({d_time}ms)\")\n",
    "\n",
    "    # labels = ['attention', 'linear', 'allreduce', 'others']\n",
    "    def plot_timesplit(prefill_record, decode_record, name, legend_stage):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\n",
    "        for i, (stage, record) in enumerate(\n",
    "            zip([\"prefill\", \"decode\"], [prefill_record, decode_record])\n",
    "        ):\n",
    "            ax = ax1 if i == 0 else ax2\n",
    "            ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "            x_pos = (\n",
    "                range(len(prefill_batch_sizes))\n",
    "                if stage == \"prefill\"\n",
    "                else range(len(decode_batch_sizes))\n",
    "            )\n",
    "            bottom = (\n",
    "                [0] * len(prefill_batch_sizes)\n",
    "                if stage == \"prefill\"\n",
    "                else [0] * len(decode_batch_sizes)\n",
    "            )\n",
    "            for idx, op in enumerate(ops):\n",
    "                ratios = (\n",
    "                    [record[bs][op] for bs in prefill_batch_sizes]\n",
    "                    if stage == \"prefill\"\n",
    "                    else [record[bs][op] for bs in decode_batch_sizes]\n",
    "                )\n",
    "                color_name = (\n",
    "                    \"silver\"\n",
    "                    if idx == 1\n",
    "                    else (\n",
    "                        \"cadetblue\"\n",
    "                        if idx == 3\n",
    "                        else \"wheat\" if idx == 2 else colors[idx]\n",
    "                    )\n",
    "                )\n",
    "                ax.bar(\n",
    "                    x_pos,\n",
    "                    ratios,\n",
    "                    bottom=bottom,\n",
    "                    label=op,\n",
    "                    color=color_name,\n",
    "                    hatch=patterns[idx],\n",
    "                    edgecolor=\"black\",\n",
    "                    alpha=opacity,\n",
    "                )\n",
    "                bottom = [sum(x) for x in zip(bottom, ratios)]\n",
    "            ax.set_xticks(x_pos)\n",
    "            (\n",
    "                ax.set_xticklabels(prefill_x_labels, fontsize=22, rotation=75)\n",
    "                if stage == \"prefill\"\n",
    "                else ax.set_xticklabels(decode_x_labels, fontsize=22, rotation=75)\n",
    "            )\n",
    "            (\n",
    "                ax.set_xlabel(\"Sequence Length\", fontsize=22, fontweight=\"bold\")\n",
    "                if stage == \"prefill\"\n",
    "                else ax.set_xlabel(\"Batch Size\", fontsize=22, fontweight=\"bold\")\n",
    "            )\n",
    "            ax.set_ylabel(\"Time (ms)\", fontsize=22, fontweight=\"bold\")\n",
    "            ax.set_title(stage.capitalize(), fontsize=22, fontweight=\"bold\")\n",
    "            ax.legend(fontsize=20, loc=\"best\", frameon=True)\n",
    "            if stage == \"prefill\":\n",
    "                ax.set_ylim([0, 0.8])\n",
    "\n",
    "        ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(name)\n",
    "        plt.show()\n",
    "    \n",
    "    plot_timesplit(prefill_record, decode_record, 'analysis_pd_breakdown.pdf', 'decode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m, in \u001b[0;36mplot\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     p_df, d_df \u001b[38;5;241m=\u001b[39m \u001b[43m_get_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     ops \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_execution_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# ops = ['attention', 'linear', 'allreduce', 'others']\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m, in \u001b[0;36m_get_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             batch_size \u001b[38;5;241m=\u001b[39m benchmark_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplica_scheduler_max_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m             decode_dfs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     46\u001b[0m                 _get_df_for_single_run(\n\u001b[1;32m     47\u001b[0m                     config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 )\n\u001b[1;32m     55\u001b[0m             )\n\u001b[0;32m---> 56\u001b[0m prefill_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefill_dfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m prefill_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     58\u001b[0m     prefill_df\u001b[38;5;241m.\u001b[39mgroupby(\n\u001b[1;32m     59\u001b[0m         [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m prefill_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefill.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sarathi-serve/env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/sarathi-serve/env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/sarathi-serve/env/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
