{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f\"{Path.cwd()}/benchmark_output\"\n",
    "MODEL_NAME = \"01-ai/Yi-34B-200K\"\n",
    "TP_DIMENSION = 2\n",
    "MAX_BATCH_SIZE = 4\n",
    "NUM_REQUESTS = 2 * MAX_BATCH_SIZE\n",
    "PREFILL_LENGTH = 1024\n",
    "DECODE_LENGTH = 16\n",
    "SEQUENCE_LENGTH = PREFILL_LENGTH + DECODE_LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_runs():\n",
    "    devices = ','.join([str(x) for x in range(TP_DIMENSION)])\n",
    "    commands = []\n",
    "    base_command = (\n",
    "        f\"CUDA_VISIBLE_DEVICES={devices} python sarathi/benchmark/main.py \\\\\\n\"\n",
    "        f\"--output_dir {OUTPUT_DIR} \\\\\\n\"\n",
    "        f\"--model_name {MODEL_NAME} \\\\\\n\"\n",
    "        f\"--model_max_model_len {SEQUENCE_LENGTH} \\\\\\n\"\n",
    "        f\"--cluster_num_replicas 1 \\\\\\n\"\n",
    "        f\"--model_tensor_parallel_degree {TP_DIMENSION} \\\\\\n\"\n",
    "        f\"--model_pipeline_parallel_degree 1 \\\\\\n\"\n",
    "        f\"--request_generator_provider synthetic \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_interval_provider static \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_num_requests {NUM_REQUESTS} \\\\\\n\"\n",
    "        f\"--synthetic_request_generator_length_provider fixed \\\\\\n\"\n",
    "        f\"--fixed_request_length_generator_prefill_tokens {PREFILL_LENGTH} \\\\\\n\"\n",
    "        f\"--fixed_request_length_generator_decode_tokens {DECODE_LENGTH} \\\\\\n\"\n",
    "        f\"--metrics_store_keep_individual_batch_metrics true \\\\\\n\"\n",
    "    )\n",
    "    vllm_command = base_command + (\n",
    "        f\"--replica_scheduler_provider vllm \\\\\\n\"\n",
    "        f\"--replica_scheduler_max_batch_size {MAX_BATCH_SIZE} \\\\\\n\"\n",
    "        f\"--vllm_scheduler_max_tokens_in_batch {SEQUENCE_LENGTH} \\\\\\n\"\n",
    "    )\n",
    "    sarathi_command = base_command + (\n",
    "        f\"--replica_scheduler_provider sarathi \\\\\\n\"\n",
    "        f\"--replica_scheduler_max_batch_size {MAX_BATCH_SIZE} \\\\\\n\"\n",
    "        f\"--sarathi_scheduler_chunk_size {PREFILL_LENGTH} \\\\\\n\"\n",
    "        f\"--sarathi_scheduler_enable_rolling_prefills true \\\\\\n\"\n",
    "        f\"--sarathi_scheduler_enable_dynamic_chunking_schedule false \\\\\\n\"\n",
    "    )\n",
    "    commands += [\n",
    "        vllm_command + f\"--metrics_store_enable_op_level_metrics false\\n\",\n",
    "        vllm_command + f\"--metrics_store_enable_op_level_metrics true\\n\",\n",
    "        sarathi_command + f\"--metrics_store_enable_op_level_metrics false\\n\",\n",
    "        sarathi_command + f\"--metrics_store_enable_op_level_metrics true\\n\",\n",
    "    ]\n",
    "    with open(f\"runs.sh\", \"w\") as f:\n",
    "        f.write(\"#!/bin/bash\\n\")\n",
    "        f.write(\"set -x\\n\")\n",
    "        for command in commands:\n",
    "            f.write(command + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_run_directories():\n",
    "    rootdir = Path(OUTPUT_DIR)\n",
    "    subdirectory_list = [\n",
    "        directory for directory in rootdir.iterdir() if directory.is_dir()\n",
    "    ]\n",
    "    return subdirectory_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_df (df: pd.DataFrame):\n",
    "    df = df[\n",
    "        ((df[\"batch_num_prefill_tokens\"] == PREFILL_LENGTH) & (df[\"batch_num_decode_tokens\"] == 0) & (df[\"batch_size\"] == 1))\n",
    "        | ((df[\"batch_num_prefill_tokens\"] == 0) & (df[\"batch_num_decode_tokens\"] == MAX_BATCH_SIZE) & (df[\"batch_size\"] == MAX_BATCH_SIZE))\n",
    "        | ((df[\"batch_num_prefill_tokens\"] == PREFILL_LENGTH - MAX_BATCH_SIZE + 1) & (df[\"batch_num_decode_tokens\"] == MAX_BATCH_SIZE - 1) & (df[\"batch_size\"] == MAX_BATCH_SIZE))\n",
    "    ]\n",
    "    df = df.groupby([\"batch_num_prefill_tokens\", \"batch_num_decode_tokens\", \"batch_size\"]).median().reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    run_directories = _get_run_directories()\n",
    "    run_directories.sort()\n",
    "\n",
    "    non_profiled_dfs = []\n",
    "    profiled_dfs = [] \n",
    "    for run_dir in run_directories:\n",
    "        try:\n",
    "            with open(\n",
    "                f\"{run_dir}/benchmark_config.yml\", \"r\"\n",
    "            ) as benchmark_config_file, open(\n",
    "                f\"{run_dir}/replica_0/batch_metrics.csv\", \"r\"\n",
    "            ) as batch_metrics_file:\n",
    "                benchmark_config = yaml.safe_load(benchmark_config_file)\n",
    "                batch_metrics = pd.read_csv(batch_metrics_file)\n",
    "\n",
    "                if benchmark_config[\"metrics_store_enable_op_level_metrics\"]:\n",
    "                    operation_metrics = pd.read_csv(f\"{run_dir}/replica_0/operation_metrics.csv\")\n",
    "                    operation_metrics = pd.merge(batch_metrics, operation_metrics, on=[\"Batch Id\"], how=\"inner\")\n",
    "                    profiled_dfs.append(operation_metrics)\n",
    "\n",
    "                non_profiled_dfs.append(batch_metrics)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"WARN: Skipping {run_dir} due to {e}\")\n",
    "    \n",
    "    non_profiled_df = _filter_df(pd.concat(non_profiled_dfs))\n",
    "    non_profiled_df[\"batch_execution_time\"] *= 1000\n",
    "    non_profiled_df[\"per_token_time\"] = non_profiled_df[\"batch_execution_time\"] / non_profiled_df[\"batch_num_tokens\"]\n",
    "    profiled_df = _filter_df(pd.concat(profiled_dfs))\n",
    "    profiled_df[\"linear\"] = sum([profiled_df[x] for x in [\"mlp_up_proj\", \"mlp_down_proj\", \"attn_pre_proj\", \"attn_post_proj\"]])\n",
    "    profiled_df[\"attention\"] = sum([profiled_df[x] for x in [\"attn\"]])\n",
    "    profiled_df = profiled_df[[\"batch_num_prefill_tokens\", \"batch_num_decode_tokens\", \"batch_size\", \"linear\", \"attention\"]]\n",
    "\n",
    "    return pd.merge(non_profiled_df, profiled_df, on=[\"batch_num_prefill_tokens\", \"batch_num_decode_tokens\", \"batch_size\"], how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_num_prefill_tokens</th>\n",
       "      <th>batch_num_decode_tokens</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>Batch Id</th>\n",
       "      <th>batch_num_tokens</th>\n",
       "      <th>batch_execution_time</th>\n",
       "      <th>inter_batch_delay</th>\n",
       "      <th>per_token_time</th>\n",
       "      <th>linear</th>\n",
       "      <th>attention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>68.122095</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>17.030524</td>\n",
       "      <td>25.500416</td>\n",
       "      <td>2.9952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>217.649878</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.212549</td>\n",
       "      <td>158.996480</td>\n",
       "      <td>3.9936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>217.235959</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.212144</td>\n",
       "      <td>160.926719</td>\n",
       "      <td>13.6704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_num_prefill_tokens  batch_num_decode_tokens  batch_size  Batch Id  \\\n",
       "0                         0                        4           4      20.0   \n",
       "1                      1021                        3           4      17.0   \n",
       "2                      1024                        0           1       3.0   \n",
       "\n",
       "   batch_num_tokens  batch_execution_time  inter_batch_delay  per_token_time  \\\n",
       "0               4.0             68.122095           0.000074       17.030524   \n",
       "1            1024.0            217.649878           0.000400        0.212549   \n",
       "2            1024.0            217.235959           0.000069        0.212144   \n",
       "\n",
       "       linear  attention  \n",
       "0   25.500416     2.9952  \n",
       "1  158.996480     3.9936  \n",
       "2  160.926719    13.6704  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_df()\n",
    "df.to_csv(\"results.csv\", index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
